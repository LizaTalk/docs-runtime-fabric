= Configure Horizontal Autoscaling (HPA) for Runtime Fabric Deployments

Runtime Fabric instances support horizontal scaling of Mule app deployments by spinning up additional replicas or spinning down unneeded replicas in response to the CPU usage as a signal.

Mulesoft supports configuring Mule apps to be responsive to the CPU usage on Kubernetes (K8s) runtime side. You can enable CPU based horizontal autoscaling per application on the Runtime Manager application deployment page, which will result in automatic scale up or scale down of the deployment replicas as needed.

In Kubernetes, a Horizontal Pod Autoscaler (HPA) automatically updates a workload resource, with the aim of automatically scaling the workload to match demand. Horizontal scaling means that the response to increased load is to deploy more pods.


== Before You Begin

To configure autoscaling, the cluster administrator must enable the https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis[support for metrics APIs^] on your managed K8s API server.
Refer to your managed vendors documentation on how to install or enable metrics API servers.


== Configuration Recommendations for Cluster Autoscaling

The following recommendations are for your infrastructure team to configure cluster autoscaling

=== Configure Node Groups

Configure one node group per availability zone, with a minimum of one node in the node group.
This ensures that Kubernetes has information about all the availability zones in the cluster and avoids the issue where zone topology unschedulable pod fails to scale-out topologySpread predicate check.

Make sure that nodes from all the node groups have the exact same labels except for the common labels and specific cloud provider labels that are different across nodes or availability zones by default.

Custom labels that are not in the following list and different across nodes need to be excluded from cluster autoscaler when evaluating similar node groups.

For reference on managed EKS labels such as:
----
eks.amazonaws.com/nodegroup
failure-domain.beta.kubernetes.io/region
failure-domain.beta.kubernetes.io/zone
kubernetes.io/hostname
topology.kubernetes.io/region
topology.kubernetes.io/zone
----


=== Configure Cluster Autoscaler Flags

Start the cluster autoscaler with flag `--balance-similar-node-groups=true` and exclude the custom labels previously mentioned in the node groups section with the flag `--balancing-ignore-label`.
These flags ensure that the cluster autoscaler balances the number of nodes across availability zones. 

For reference on managed EKS cluster, the following node labels are different across node groups:

----
k8s.amazonaws.com/eniConfigCustom
vpc.amazonaws.com/eniConfig
----

Therefore, the cluster autoscaler starts with:

----
- --balancing-ignore-label=k8s.amazonaws.com/eniConfigCustom
- --balancing-ignore-label=vpc.amazonaws.com/eniConfig
----

=== Configure Overprovisioning Pod

Configure at least one overprovisioning pod per availability zone. Each overprovisioning pod holds a resource that can accommodate three Mule app pods, which improves the performance when new nodes are needed.



[NOTE]
Refer to your managed vendor documentation for cluster autoscaling best practices and over-provisioning configuration details.


=== Configure Topology Spread Constraints

Your Mule app deployments which are enabled with autoscaling, are required to customize the kubernetes resources for the topology spread constraints in pod spec by updating the *whenUnsatisfiable* field with `DoNotSchedule`.
This configuration ensures that autoscaling replicas are evenly spread across zones for high availability.

----
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        rtf.mulesoft.com/id: {{ .Values.id }}
----

For further details on the configuration, refer to xref:customize-kubernetes-crd.adoc[] documentation.

[WARNING]
Customizing topology spread with `whenUnsatisfiable: DoNotSchedule`, without the recommended configurations for node groups per availability zone, cluster autoscaler, and overprovisionining, can lead to K8s failing to schedule the replicas with `Pending state` error.

=== Understand Autoscaling Policy

MuleSoft owns and applies the https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/[autoscaling^] policy for your Mule application deployments.

The CPU based HPA policy used for all Mule apps deployed on Runtime Fabric instances, is as follows:

----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app
  namespace: app-namespace
spec:
  behavior:
    scaleDown:
      policies:
      - periodSeconds: 15
        type: Percent
        value: 100
      selectPolicy: Max
      stabilizationWindowSeconds: 300
    scaleUp:
      policies:
      - periodSeconds: 180
        type: Percent
        value: 100
      selectPolicy: Max
      stabilizationWindowSeconds: 0
  maxReplicas: 3
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 70
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app

----

Some points to consider:

* Scale up: Each period up to 100% of the running pods may be added until the maximum configured replicas.
* Scale down: Each period, running pods may be scaled down till it reaches the minimum configured replicas.
* The number of pods added is based on the aggregated calculations over the past 180 seconds.
* The number of pods removed is based on the aggregated calculations over the past 300 seconds.
* Max scale up profile is 1 -> 2 -> 4 -> 8, where MuleSoft hits 8 replicas in approximately 6 minutes.


== Performance Considerations and Limitations

For a successful horizontal autoscaling of your Mule apps, review the following performance considerations:

* In Runtime Fabric, the policy in use was benchmarked for Mule apps with CPU Reserved: 0.45vCpu and Limit: 0.55vCpu, which corresponds to these settings:
+
----
        resources:
          limits:
            cpu: 550m
          requests:
            cpu: 450m
----
+
* Mule apps that scale based on CPU usage are a good fit with CPU based HPA. For example, HTTP/HTTPS apps with async requests, 'B', 'C'.

* Scale up and scale down performance can vary based on the replica size and the application type.
=== Limitations
Horizontal autoscaling does not work with clustering and rate limiting.


== Configure Horizontal Autoscaling

To configure horizontal autoscaling for Mule apps deployed to Runtime Fabric, follow these steps:

. Enable the https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis[support for metrics APIs^] on your managed K8s API server.
. From Anypoint Platform, select *Runtime Manager* > *Applications*.
. Click *Deploy application*.
. In the *Runtime* tab, check the *Enable Autoscaling* box.
. Set the minimum and maximum *Replica Count* limits.
. Click *Deploy Application*.

image::rtf-autoscaling.png[Runtime Manager UI with Enable Autoscaling field selected]



== Autoscaling Status and Logs

When an autoscaling event occurs and your Mule app with horizontal autoscaling scales up, you can check the *Scaling* status in the Runtime Manager UI by clicking *View status* in your applicationâ€™s details window:

image::rtf-autoscaling-status.png[Runtime Manager UI with Mule app and Scaling status]

For beta, MuleSoft does not support autoscaling logs for Runtime Fabric. You can track the scaled up replicas startup and the number of replicas of your Mule apps by running the following `kubectl` command in your terminal:

[source,console,linenums]
----
kubectl get events | grep HorizontalPodAutoscaler
----

Use the `kubectl get events` command in Kubernetes to retrieve events that occurred within the cluster. The command provides information about various activities and changes happening in the cluster, such as pod creations, deletions, and other important events.

The `grep` command filters the output of `kubectl get events` for lines that contain the string `HorizontalPodAutoscaler`. The following example shows the command output that includes events related to a `HorizontalPodAutoscaler` with information about successful rescaling operations triggered by the HPA:

[source,console,linenums]
----
# kubectl get events | grep HorizontalPodAutoscaler
5m20s  Normal SuccessfulRescale   HorizontalPodAutoscaler   New size: 4; reason: cpu resource utilization (percentage of request) above target
5m5s   Normal SuccessfulRescale   HorizontalPodAutoscaler   New size: 8; reason: cpu resource utilization (percentage of request) above target
4m50s  Normal SuccessfulRescale   HorizontalPodAutoscaler   New size: 10; reason:
----
 
