= View and Configure Logging in Runtime Fabric
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Runtime Fabric generates log files for the following entities:

* Deployed Mule applications
* Deployed API proxies
* Runtime Fabric services
* Kubernetes services

== Log Levels

Runtime Fabric enables you to specify the severity level of messages that are written to the log file.

[%header,cols="3*a"]
.Runtime Fabric Log Levels
|===
| Value
| Description
| Command

| All Priorities
| List all messages.
| N/A

| ERROR
| List only error messages, such when an exception occurs.
| priority:ERROR

| FATAL
| List only fatal messages when an application fails.
| priority:FATAL

| INFO
| List informative messages.
| priority:INFO

| SYSTEM
| List messages about application and worker startup.
| priority:SYSTEM

| CONSOLE
| List messages about console events such as setting the object store.
| priority:CONSOLE

| WARN
| List warning messages.
| priority:WARN

| DEBUG
| List debugging messages.
| priority:DEBUG
|===

Log levels are specified per Mule application or API proxy during deployment.

[WARNING]
Because log levels are specified when deploying a Mule application or an API proxy, you cannot change the log levels after deployment.

== View Logs from an Application

Ops Center shows a stream of logs generated by applications and services running on Runtime Fabric. This is useful when log forwarding is not configured.  

. From Ops Center, navigate to *Logging*.
. Select *Kubernetes*.
. Select the *Pods* tab.
. In the drop-down list adjacent to the search input area, select the ID of the environment where the application is deployed.
. Locate the pod name that starts with the name of your application.
. Select that pod name and then select *Logs*.

The *Logs* tab is displayed with a filter applied to your application.

[NOTE]
To view the latest logs, select *Refresh*.

=== Filters

There are two levels of filters for drilling down in the logs:

* The `_Containers_` filter filters by container name.
* The `_Pods_` filter filters by pod name.
+
This is useful for specifying application names that are followed by a wildcard (`%`).

== Forward Logs to External Services

Runtime Fabric supports log forwarding to:

* Anypoint Monitoring
* Third-party logging solutions using Fluent Bit log forwarding output plugins:

** Azure Log Analytics
** Elasticsearch
** Graylog Extended Log Format (GELF)
** Splunk

[NOTE]
Only one external log forwarding configuration is supported at any one time. However, you can enable Anypoint log forwarding and external log forwarding service at the same time.

=== Enable Log Forwarding

. Install and configure the applicable third-party logging solution. Verify that you have the required resources allocated. Refer to the documentation for your third-party logging solution for details. 
. From Anypoint Platform, select *Runtime Manager*.
. Select *Runtime Fabrics* in the sidebar navigation.
. Select the applicable Runtime Fabric based on the name used during installation.
. Select the *Log Forwarding* tab.
. Select the *Forward to Anypoint Monitoring (Requires Titanium)* checkbox if you have a Titanium subscription and you want to forward logs to Anypoint Monitoring.
. Select the *Forward to other logging provider* checkbox to forward logs using a Fluent Bit log forwarding output plugin.
.. In the *Connects to* drop-down list, select the applicable third-party log forwarding solution.
.. Enter Fluent Bit configuration information for your third-party logging solution, as shown in the following sections:

*** <<Azure Log Analytics Configuration Parameters, Azure Log Analytics Configuration Parameters>> 
*** <<Elasticsearch Configuration Parameters, Elasticsearch Configuration Parameters>>
*** <<GELF Configuration Parameters, GELF Configuration Parameters>>
*** <<Splunk Configuration Parameters, Splunk Configuration Parameters>>

.. Select *Submit*.
.. To verify successful log forwarding, manually check the logs using the external log forwarding service.

==== Azure Log Analytics Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Azure Log Analytics Configuration Parameters
|===
| Key | Description | Required | Default Value
| Customer_ID | Specifies the customer ID or workspace ID string. | Yes |
| Shared_Key | Specifies the primary or secondary connected sources client authentication key. | Yes |
| Log_Type | Specifies the event type name. | No | `fluentbit`
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name        azure
    Match       *
    Customer_ID id
    Shared_Key  key
```

==== Elasticsearch Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Elasticsearch Configuration Parameters
|===
| Key | Description | Required | Default Value
| Host | Specifies the IP address or hostname of the target Elasticsearch instance. | Yes | 127.0.0.1
| Port | Specifies the TCP port of the target Elasticsearch instance. | Yes | 9200
| Index | Specifies the index name. | Yes | fluentbit
| Path | Elasticsearch accepts new data on HTTP query path `/_bulk`. But it is also possible to serve Elasticsearch behind a reverse proxy on a subpath. This option defines such a path on the Fluent Bit side. It adds a path prefix in the indexing HTTP POST URI. | No | Empty string
| Buffer_Size | Specifies the buffer size used to read the response from the Elasticsearch HTTP service. This option is useful for debugging purposes where reading the full response is needed. Note that the size of the response grows depending on the number of records inserted. To specify an unlimited amount of memory, set this value to `False`. Otherwise, set the value according to the Unit Size specification. | No | 4KB
| HTTP_User | Specifies an optional username credential for Elastic X-Pack access. | No | 
| HTTP_Passwd | Specifies a password for the user defined in `HTTP_User`. | No | 
| Type | Specifies the type name. | No | flb_type
| Logstash_Format | Enables Logstash format compatibility. This option takes a Boolean value: `True`, `False`, `On`, or `Off`. | No |
| Logstash_Prefix | When `Logstash_Format` is enabled, the `Index` name is composed of a prefix and the date. For example, if `Logstash_Prefix` is set to `mydata`, your index becomes `mydata-YYYY.MM.DD`. The last string that is appended belongs to the date when the data is generated. | No | logstash
| Logstash_DateFormat | Specifies the time format (based on `strftime`) that is used to generate the second part of the `Index` name. | No | %Y.%m.%d
| Time_Key | When `Logstash_Format` is enabled, each record is assigned a new timestamp field. The `Time_Key` property defines the name of that field. | No | @timestamp
| Time_Key_Format | When `Logstash_Format` is enabled, this property defines the format of the timestamp. | No | %Y-%m-%dT%H:%M:%S
| Include_Tag_Key | When enabled, the tag name is appended to the record. | No | Off
| Tag_Key | When `Include_Tag_Key` is enabled, this property defines the key name for the tag. | No | _flb-key
| Generate_ID | When enabled, generates the `_id` value for outgoing records. This prevents duplicate records when retrying ES. | No | Off
| Replace_Dots | When enabled, replaces field name dots ('.') with an underscore ('_'), which is required by Elasticsearch 2.0 through 2.3. | No | Off
| Trace_Output | When enabled, prints the Elasticsearch API calls to stdout (for diagnosis). | No | Off
| Trace_Error | When enabled, prints the Elasticsearch API calls to stdout when Elasticsearch returns an error. | No | Off
| Current_Time_Index | Specifies using the current time for index generation instead of message record information. | No | Off
| Logstash_Prefix_Key | Prefixes keys with this string. | No |
| tls | Enables or disables TLS support. | No | Off
| tls.ca_file | Specifies a CA certificate file (required when `tls` is enabled) | No |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name  es
    Match *
    Host  192.168.2.3
    Port  9200
    Index my_index
    Type  my_type
```

==== GELF Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.GELF Configuration Parameters
|===
| Key | Description | Required | Default Value
| Match | Specifies the pattern to match for log tags to be output by this plugin. | Yes |
| Host | Specifies the IP address or hostname of the target Graylog server. | Yes | 127.0.0.1
| Port | Specifies the port on which your Graylog GELF input is listening. | Yes | 12201
| Mode | Specifies the protocol to use (`tls`, `tcp`, or `udp`). | Yes | udp
| Gelf_Short_Message_Key | Specifies a short descriptive message (This must be set in GELF). | Yes | log
| Gelf_Timestamp_Key | Specifies your log timestamp (This should be set in GELF). | No | timestamp
| Gelf_Host_Key | Specifies the key for the value that is used as the name of the host, source, or application that sent the message. (This must be set in GELF). | No | host
| Gelf_Full_Message_Key | Specifies the key to use as the long message, which can contain a backtrace. (This is optional in GELF.) | No | full_message
| Gelf_Level_Key | Specifies the key to be used as the log level. Its value must be a standard syslog level (between 0 and 7). | No | level
| Packet_Size | Specifies the size of packets to be sent if the transport protocol is set to `udp`. | No | 1420
| Compress | Specifies compression of your UDP packets if the transport protocol is set to `udp`. | No | true
| tls | Enables or disables TLS support. | No | Off
| tls.ca_file | Specifies a CA certificate file (required when `tls` is enabled) | No |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name                    gelf
    Match                   kube.*
    Host                    <your-graylog-server>
    Port                    12201
    Mode                    tcp
    Gelf_Short_Message_Key  data
```

==== Splunk Configuration Parameters

[%header%autowidth.spread,cols="a,a,a,a"]
.Splunk Configuration Parameters
|===
| Key | Description | Required | Default Value
| Host | Specifies the IP address or hostname of the target Splunk service. | Yes | 127.0.0.1
| Port | Specifies the TCP port of the target Splunk service. | Yes | 8088
| Splunk_Token | Specifies the Authentication Token for the HTTP Event Collector interface. | Yes |
| Splunk_Send_Raw | When enabled, specifies the record keys and values to be set in the top level of the map instead of nested under the `event` key. | No | Off
| HTTP_User | Specifies an optional username for basic authentication on the HTTP Event Collector. | No |
| HTTP_Passwd | Specifies a password for the user defined in `HTTP_User`. | No |
| tls | Enables or disables TLS support. | No | Off
| tls.ca_file | Specifies a CA certificate file (required when `tls` is enabled) | No |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name        splunk
    Match       *
    Host        127.0.0.1
    Port        8088
    TLS         On
    TLS.Verify  Off
    Message_Key my_key
```

== Disable Anypoint Monitoring or Fluent Bit Log Forwarding

. Navigate to Runtime Manager and select *Applications*.
. Select the application for which you want to disable log forwarding.
. Select the *Logs* tab.
. Deselect the applicable log forwarding options:
+
** *Forward logs to Anypoint Monitoring* 
** *Forward logs to <third-party service>*

[WARNING]
Disabling the third-party log forwarding option deletes all configuration information provided in the *Runtime Fabrics* page in Runtime Manager.

Application level configuration is not currently supported. You cannot enable or disable application level log forwarding for an external log forwarding service. 

== Forward Logs to Other External Services

Anypoint Runtime Fabric also enables you to forward application and cluster logs to external logging solutions other than the third-party solutions supported using Fluent Bit log forwarding output plugins. The log forwarder (`rsyslog` client service) built into Runtime Fabric enables you to send log data to an `rsyslog` server over TCP or UDP for viewing, retention, and receiving alerts in a centralized destination. Logging solutions such as Logstash provide methods to receive log data from `rsyslog` clients.

Anypoint Runtime Fabric provides dashboards and alerts for critical metrics when performance or availability is compromised.  An SMTP server is required to receive alerts. To view and configure Anypoint Runtime Fabric dashboards and alerts using Ops Center:

. Using a terminal, open a shell/SSH connection to a controller VM.
. Create a file named `log-forwarder.yaml`.
. Add the following content to this file after customizing based on the table below:
+
----
kind: logforwarder
version: v2
metadata:
   name: log-forwarder
spec:
   address: 192.168.100.1:514
   protocol: udp
----
+
Use the following values as applicable to your environment:
+
[%header,cols="2*a"]
.Log Forwarding Configuration Parameters
|===
|Key | Description
|`name` | Specifies the name of the log forwarding rule.
|`address` | Specifies the endpoint and port to forward the log data.
|`protocol` | Specifies the protocol to send the data to. Supported protocols are TCP or UDP.
|===
+
. Run the following command on the controller VM, referencing the file created earlier.
----
gravity resource create log-forwarder.yaml
----

Your logs are forwarded to your external logging solution.

== See Also

* xref:configure-alerting.adoc[Configure Alerting on Anypoint Runtime Fabric]
* xref:deploy-to-runtime-fabric.adoc[Deploy a Mule Application to a Runtime Fabric]
