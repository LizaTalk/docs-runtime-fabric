= View and Configure Logging in Runtime Fabric
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Runtime Fabric generates log files that provide information about the following:

* Deployed Mule applications
* Deployed API proxies
* Runtime Fabric services
* Kubernetes services

== Log Levels

Runtime Fabric enables you to specify the level of severity of the message written to the log file.

[%header,cols="3*a"]
.Runtime Fabric Log Levels
|===
| Value
| Description
| Command

| All Priorities
| List all messages
| N/A

| ERROR
| List only error messages, such when an exception occurs.
| priority:ERROR

| FATAL
| List only fatal messages for when an application fails
| priority:FATAL

| INFO
| List informative messages
| priority:INFO

| SYSTEM
| List messages about application and worker startup
| priority:SYSTEM

| CONSOLE
| List message about console events such as setting the objectstore
| priority:CONSOLE

| WARN
| List warning messages
| priority:WARN

| DEBUG
| List debugging messages
| priority:DEBUG
|===

Log levels are specified per Mule application or API proxy during deployment.

[WARNING]
Log levels are specified when deploying a Mule application or an API proxy. After deployment you cannot change the log levels.

== View Logs from an Application

Ops Center shows a stream of logs generated by applications and services running on Runtime Fabric. This can be useful when log forwarding is not set up.  

. From Ops Center, navigate to *Logging*.
. Select *Kubernetes*.
. Select the *Pods* tab.
. In the drop-down list adjacent to the search input area, select the environment ID where the application was deployed.
. Locate the pod name that starts with the name of your application.
. Select that pod name and then select *Logs*.

The page should redirect to the *Logs* tab with a filter applied to your application.

[NOTE]
To view the latest logs, select *Refresh*.

=== Filters

There are two levels of filters for drilling down in the logs:

* _Containers_ filter for filtering by container name.
* _Pods_ filter for filtering by pod name. This is useful for specifying application names that are followed by a wildcard (`%`).

== Forward Logs to External Services

Runtime Fabric supports log forwarding to:

* Anypoint Monitoring
* The following third party logging services using Fluent Bit log forwarding output plugins:

** Azure
** CloudWatch
** ELK
** Graylog Extended Log Format (GELF)
** Splunk
** Syslog
+
[NOTE]
Log data can be delivered to multiple destinations. However, only one Fluent Bit log forwarding output plugin is supported at any one time. Multiple log forwarding output configurations are not supported. For example, you can forward logs to Anypoint Monitoring and a third party logging solution, but not to two third party logging solutions.

=== Enable Third Party Log Forwarding

. Install and configure the applicable third-party logging service, and verify that you have allocated the required resources. Refer to the documentation for your third party logging solution for details. 
. Navigate to Runtime Manager and select *Runtime Fabrics*.
. Select the name of your Runtime Fabric to open the management page.
. Select the *Log Forwarding* tab.
. Select *Forward to Anypoint Monitoring (Requires Titanium)* if you have a Titanium subscription and you want to forward logs to Anypoint Monitoring.
. Select *Forward to other logging provider* to forward logs using a Fluent Bit log forwarding output plugin.
.. In the *Connects to* drop-down list, select the applicable third party log forwarding solution.
.. Enter the requested Fluent Bit configuration information: (link to each section for each 3rd party)?????.

*** <<Azure Log Analytics Configuration Parameters, Azure Log Analytics Configuration Parameters>>
*** <<Amazon CloudWatch Configuration Parameters, Amazon CloudWatch Configuration Parameters>>
*** <<ELK Configuration Parameters, ELK Configuration Parameters>>
*** <<GELF Configuration Parameters, GELF Configuration Parameters>>
*** <<Splunk Configuration Parameters, Splunk Configuration Parameters>>
*** <<Syslog Configuration Parameters, Syslog Configuration Parameters>>

.. Select *Submit*.
.. To verify successful log forwarding, manually check the logs using your third party log forwarding service.

==== Azure Log Analytics Configuration Parameters

The Fluent Bit Azure output plugin allows you to ingest your records into Azure Log Analytics service.

[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters
|===
| Key | Description | Required | Default value
| Customer_ID | Specifies the customer ID or WorkspaceID string. | Yes |
| Shared_Key | Specifies the primary or secondary Connected Sources client authentication key. | Yes |
| Log_Type | Specifies the event type name. | No | `fluentbit`
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name        azure
    Match       *
    Customer_ID id
    Shared_Key  key
```

==== Amazon CloudWatch Configuration Parameters
[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters
|===
| Key | Description | Required | Default value
| `region`| The AWS region | |
| `log_group_name` | Specifies the name of the CloudWatch Log Group to which you want log records sent | |
| `log_stream_name`| Specifies the name of the CloudWatch Log Stream to which you want log records sent | |
| `log_stream_prefix`| Specifies the prefix for the Log Stream name. The tag is appended to the prefix to construct the full log stream name. Not compatible with the `log_stream_name` option | |  
| `log_key`| By default, the whole log record is sent to CloudWatch. If you specify a key name with this option, then only the value of that key is sent to CloudWatch. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message is sent to CloudWatch. | |
| `log_format`| An optional parameter that specifies the format of the data for identification by CloudWatch. A value of `json/emf` enables CloudWatch to extract custom metrics embedded in a JSON payload. See the [Embedded Metric Format](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html). | |
| `role_arn`| Specifies the ARN of an IAM role to assume (for cross account access). | |
| `auto_create_group`| Automatically creates the log group. Valid values are "true" or "false" (case insensitive). Defaults to `false`. | |
| `endpoint`| Specifies a custom endpoint for the CloudWatch Logs API. | |
| `credentials_endpoint`| Specifies a custom HTTP endpoint from which to pull credentials. | | 
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name cloudwatch
    Match   *
    region us-east-1
    log_group_name fluent-bit-cloudwatch
    log_stream_prefix from-fluent-bit-
    auto_create_group true
```
==== ELK Configuration Parameters
[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters
|===
| Key | Description | Required | Default value
| Host | Specifies the IP address or hostname of the target Elasticsearch instance | | 127.0.0.1
| Port | Specifies the TCP port of the target Elasticsearch instance | | 9200
| Path | Elasticsearch accepts new data on HTTP query path "/_bulk". But it is also possible to serve Elasticsearch behind a reverse proxy on a subpath. This option defines such a path on the fluent-bit side. It adds a path prefix in the indexing HTTP POST URI. | | Empty string
| Buffer_Size | Specifies the buffer size used to read the response from the Elasticsearch HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to False, otherwise the value must be according to the Unit Size specification. | | 4KB
| Pipeline | Newer versions of Elasticsearch allow setup filters called pipelines. This option specifies which pipeline the database should use. For performance reasons, is strongly recommended to perform parsing and filtering on Fluent Bit side and avoid pipelines. | | 
| HTTP_User | Specifies an optional username credential for Elastic X-Pack access. | | 
| HTTP_Passwd | Specifies a password for the user defined in `HTTP_User`. | | 
| Index | Specifies the index name. | | fluentbit
| Type | Specifies the type name. | | flb_type
| Logstash_Format | Enables Logstash format compatibility. This option takes a boolean value: True/False, On/Off
Off | |
| Logstash_Prefix | When Logstash_Format is enabled, the `Index` name is composed of a prefix and the date, for example: If Logstash_Prefix is set to 'mydata', your index becomes `mydata-YYYY.MM.DD`. The last string appended belongs to the date when the data is being generated. | | logstash
| Logstash_DateFormat | Specifies the time format (based on strftime) that is used to generate the second part of the `Index` name. | | %Y.%m.%d
| Time_Key | When Logstash_Format is enabled, each record is assigned a new timestamp field. `The Time_Key` property defines the name of that field. | | @timestamp
| Time_Key_Format | When Logstash_Format is enabled, this property defines the format of the timestamp. | | %Y-%m-%dT%H:%M:%S
| Include_Tag_Key | When enabled, appends the Tag name to the record. | | Off
| Tag_Key | When `Include_Tag_Key` is enabled, this property defines the key name for the tag. | | _flb-key
| Generate_ID | When enabled, generates the _id for outgoing records. This prevents duplicate records when retrying ES. | | Off
| Replace_Dots | When enabled, replaces field name dots with an underscore, which is required by Elasticsearch 2.0-2.3. | | Off
| Trace_Output | When enabled, prints the elasticsearch API calls to stdout (for diag only). | | Off
| Trace_Error | When enabled, prints the elasticsearch API calls to stdout when elasticsearch returns an error. | | Off
| Current_Time_Index | Specifies using the current time for index generation instead of the message record. | | Off
| Logstash_Prefix_Key | Prefixes keys with this string. | |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name  es
    Match *
    Host  192.168.2.3
    Port  9200
    Index my_index
    Type  my_type
```

==== GELF Configuration Parameters
[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters. There are some mandatory and optional fields which are used by Graylog in GELF format. These fields are determined with Gelf\*_Key_ key in this plugin.???
|===
| Key | Description | Required | Default value
| Match | Specifies the pattern to match for tags of logs to be outputted by this plugin. | |
| Host | Specifies the IP address or hostname of the target Graylog server. | | 127.0.0.1
| Port | Specifies the port on which your Graylog GELF input is listening. | | 12201
| Mode | Specifies the protocol to use (tls, tcp or udp). | | udp
| Gelf_Short_Message_Key | Specifies a short descriptive message (MUST be set in GELF). | | short_message
| Gelf_Timestamp_Key | Specifies your log timestamp (SHOULD be set in GELF). | | timestamp
| Gelf_Host_Key | Specifies the key for which the value is used as the name of the host, source, or application that sent the message. (MUST be set in GELF). | | host
| Gelf_Full_Message_Key | Specifies the key to use as the long message that can contain a backtrace. (Optional in GELF) | |full_message
| Gelf_Level_Key | Specifies the key to be used as the log level. Its value must be in standard syslog levels (between 0 and 7). | (Optional in GELF) | level
| Packet_Size | Specifies the size of packets to be sent if the transport protocol is udp. | | 1420
| Compress | Specifies compression of your UDP packets if the transport protocol is udp. | | true
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name                    gelf
    Match                   kube.*
    Host                    <your-graylog-server>
    Port                    12201
    Mode                    tcp
    Gelf_Short_Message_Key  data
```

==== Splunk Configuration Parameters
[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters
|===
| Key | Description | Required | Default value
| Host | Specifies the IP address or hostname of the target Splunk service. | | 127.0.0.1
| Port | Specifies the TCP port of the target Splunk service. | | 8088
| Splunk_Token | Specifies the Authentication Token for the HTTP Event Collector interface. | |
| Splunk_Send_Raw | When enabled, specifies the record keys and values to be set in the top level of the map instead of under the `event` key. | | Off
| HTTP_User | Specifies an optional username for Basic Authentication on HEC. | |
| HTTP_Passwd | Specifies a password for the user defined in `HTTP_User`. | |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
    Name        splunk
    Match       *
    Host        127.0.0.1
    Port        8088
    TLS         On
    TLS.Verify  Off
    Message_Key my_key
```

==== Syslog Configuration Parameters
[%header%autowidth.spread,cols="a,a"]
.Configuration Parameters
|===
| Key | Description | Required | Default value
| Match | Specifies the pattern to match for tags of logs to be outputted by this plugin. | |
| Host| Specifies the IP address or hostname of the target syslog server. | |	127.0.0.1
| Port | Specifies the port on which your syslog server is listening. | | 514
| Mode | Specifies the protocol to use ( udp, tcp or tls). | | udp
| syslog_format | Specifies the syslog format: `rfc3164` or `rfc5424`. | | rfc5424
| syslog_maxsize | Specifies the maximum message size. | | 2048 for rfc5424 or 1024 for rfc3164
| syslog_severity_key | Specifies the key to use for the syslog severity. | | 6 (info)
| syslog_facility_key | Specifies the key to use for the syslog facility. | | 1 (user)
| syslog_hostname_key | Specifies the key to use for the name of the host. | |
| syslog_appname_key | Specifies the key to use for the application name. | |	
| syslog_procid_key | Specifies the key to use for the process identifier. | |	
| syslog_msgid_key | Specifies the key to use for message ID. | |	
| syslog_message_key | Specifies the key to use for the syslog message.	| |	
| syslog_sd_key | Specifies the key to syslog structured data, which can be set multiple times. This must be a map. | |
|===

Example Fluent Bit output plugin configuration:
```
[OUTPUT]
	Name   syslog
	Match  *

	Host   127.0.0.1
	Port   514
	Mode   tcp

	Syslog_Format        rfc5424
	Syslog_Severity_key  serverity
	Syslog_Facility_key  factility
	Syslog_Hostname_key  hostname
	Syslog_Appname_key   app
	Syslog_Procid_key    pid
	Syslog_Message_key   log
	Syslog_sd_key        info=]1
	Syslog_sd_key        info2
```

== Disable Log Forwarding for Specific Pods

By default, Fluent Bit log forwarding is enabled on all pods (MuleApplication pods, Runtime Fabric pods and Infrastructure pods). 

To disable log forwarding for specific pods, you can annotate it with `fluentbit.io/exclude`, as shown in the following example:
```
apiVersion: v1
kind: Pod
metadata:
  name: apache-logs
  labels:
    app: apache-logs
  annotations:
    fluentbit.io/exclude: "true"
spec:
  containers:
  - name: apache
  ...
  ```
OR, you can use the following kubectl command to annotate it on pods in a specific namespace:
```
kubectl annotate pod <pod-name> -n <namespace> fluentbit.io/exclude="true"
```
Note that the annotation value is boolean, and the boolean value must be in quotation marks. 

== Disable Anypoint Monitoring or Fluent Bit Log Forwarding:

. Navigate to Runtime Manager and select *Applications*.
. Select the application for which you want to disable log forwarding.
. Select the *Logs* tab.
. Deselect the applicable log forwarding options:
+
** *Forward logs to Anypoint Monitoring* 
** *Forward logs to <third party service>*
+
[WARNING]
Disabling log forwarding options deletes all the applicable configuration information from Runtime Fabric.

== Forward Logs to Other External Services

Anypoint Runtime Fabric enables you to forward application and cluster logs to an external logging service. The log forwarder built in to Runtime Fabric enables you to send log data to an `rsyslog` server over TCP or UDP.

Log data from Anypoint Runtime Fabric components and Mule applications can be forwarded to an external logging solution for viewing, retention and alerting in a centralized destination. An `rsyslog` client service is included in Runtime Fabric, and provides log forwarding transmission via TCP or UDP to an `rsyslog` server. Logging services such as Splunk or Logstash provide methods to receive log data from `rsyslog` clients.

Anypoint Runtime Fabric provides dashboards and alerts on critical metrics when performance or availability are compromised. These can be viewed and adjusted using Ops Center. An SMTP server is required to receive alerts.

. Using a terminal, open a shell/SSH connection to a controller VM.
. Create a file named `log-forwarder.yaml`.
. Add the following content to this file after customizing based on the table below:
+
----
kind: logforwarder
version: v2
metadata:
   name: log-forwarder
spec:
   address: 192.168.100.1:514
   protocol: udp
----
+
Using the following values specific to your environment:
+
[%header,cols="2*a"]
.Log Forwarding Configuration Parameters
|===
|Key | Description
|`name` | Specifies the name of the log forwarding rule.
|`address` | Specifies the endpoint and port to forward the log data.
|`protocol` | Specifies the protocol to send the data to. Supported protocols are TCP or UDP.
|===
+
. Run the following command on the controller VM, referencing the file created earlier.
----
gravity resource create log-forwarder.yaml
----

Your logs should now be forwarded to your external logging service.

== See Also

* xref:configure-alerting.adoc[Configure Alerting on Anypoint Runtime Fabric]
* xref:deploy-to-runtime-fabric.adoc[Deploy a Mule Application to a Runtime Fabric]
